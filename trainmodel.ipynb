{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型h5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对，步骤4的模型训练不应该直接使用步骤3中生成的特征向量（即BERT特征矩阵）。步骤4应该是基于您手动标记的数据集（即Excel文件）来训练模型。一旦模型训练完成，就可以将其应用于新的RFP文件中提取的文本（这些文本需要经过与训练数据相同的预处理和特征提取流程）。\n",
    "\n",
    "以下是根据您提供的信息修改后的步骤4模型训练代码的大致结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BERT features: 6\n",
      "Number of labels: 134\n"
     ]
    }
   ],
   "source": [
    "# 检查BERT特征和标签的数量是否匹配\n",
    "print(\"Number of BERT features:\", bert_features.shape[0])\n",
    "print(\"Number of labels:\", len(categorical_labels))\n",
    "\n",
    "# 如果数量不匹配，检查CSV文件的加载过程\n",
    "# 确保CSV文件中的行数与您的标签数量相匹配\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了一个全连接的神经网络模型，可以处理768维的BERT特征。\n",
    "加入了Dropout层，以帮助防止过拟合。\n",
    "使用了softmax激活函数的输出层，其大小与分类标签的数量相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT features shape: (6, 768)\n",
      "Categorical labels shape: (134, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT features shape:\", bert_features.shape)\n",
    "print(\"Categorical labels shape:\", categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BERT features: 117\n"
     ]
    }
   ],
   "source": [
    "bert_features_matrix = np.load('/Users/xiao1/Desktop/good and bad/bert_features_matrix.npy')\n",
    "print(\"Number of BERT features:\", bert_features_matrix.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indices: {128, 129, 130, 131, 132, 133, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\n"
     ]
    }
   ],
   "source": [
    "missing_indices = set(df.index) - set(df.index[df.index.isin(range(bert_features_matrix.shape[0]))])\n",
    "print(\"Missing indices:\", missing_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 1.8911 - accuracy: 0.0000e+00 - val_loss: 0.9366 - val_accuracy: 0.7368\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3023 - accuracy: 0.5946 - val_loss: 1.0089 - val_accuracy: 0.7368\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2283 - accuracy: 0.5946 - val_loss: 1.1599 - val_accuracy: 0.7368\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1276 - accuracy: 0.5946 - val_loss: 1.3775 - val_accuracy: 0.7368\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1474 - accuracy: 0.6351 - val_loss: 1.3338 - val_accuracy: 0.7368\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0918 - accuracy: 0.6216 - val_loss: 1.2433 - val_accuracy: 0.7368\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0568 - accuracy: 0.5946 - val_loss: 1.2387 - val_accuracy: 0.7368\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0525 - accuracy: 0.5946 - val_loss: 1.2742 - val_accuracy: 0.7368\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0103 - accuracy: 0.6081 - val_loss: 1.3524 - val_accuracy: 0.7368\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9740 - accuracy: 0.6351 - val_loss: 1.4285 - val_accuracy: 0.6842\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2636 - accuracy: 0.5417\n",
      "Test Loss: 1.263647437095642\n",
      "Test Accuracy: 0.5416666865348816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/xiao1/Desktop/label_encoder.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# 加载BERT特征向量\n",
    "bert_features_matrix = np.load('/Users/xiao1/Desktop/good and bad/bert_features_matrix.npy')\n",
    "\n",
    "# 加载标签数据\n",
    "df = pd.read_excel('/Users/xiao1/Desktop/LDMar10.xlsx')\n",
    "\n",
    "# 处理缺失样本\n",
    "missing_indices = {128, 129, 130, 131, 132, 133, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\n",
    "df = df.drop(index=missing_indices)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['Level'])\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bert_features_matrix, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(768,)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {evaluation[0]}')\n",
    "print(f'Test Accuracy: {evaluation[1]}')\n",
    "\n",
    "# Save model and label encoder\n",
    "model.save('/Users/xiao1/Desktop/my_trained_model.h5')\n",
    "joblib.dump(label_encoder, '/Users/xiao1/Desktop/label_encoder.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上面是正确的一版本带降维度bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 1.8309 - accuracy: 0.0824 - val_loss: 1.6942 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4998 - accuracy: 0.7647 - val_loss: 1.5657 - val_accuracy: 0.6818\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3634 - accuracy: 0.7765 - val_loss: 1.5375 - val_accuracy: 0.6818\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2426 - accuracy: 0.8000 - val_loss: 1.5021 - val_accuracy: 0.6818\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1304 - accuracy: 0.8941 - val_loss: 1.5101 - val_accuracy: 0.6818\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0296 - accuracy: 0.9176 - val_loss: 1.4447 - val_accuracy: 0.7273\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9230 - accuracy: 0.9176 - val_loss: 1.6139 - val_accuracy: 0.2727\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9849 - accuracy: 0.9176 - val_loss: 1.3739 - val_accuracy: 0.7273\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8531 - accuracy: 0.9176 - val_loss: 1.4100 - val_accuracy: 0.6364\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7737 - accuracy: 0.9529 - val_loss: 1.2914 - val_accuracy: 0.7273\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0342 - accuracy: 0.8519\n",
      "Test Loss: 1.0341757535934448\n",
      "Test Accuracy: 0.8518518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/xiao1/Desktop/label_encoder.pkl']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 确保已下载NLTK数据包\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    # 移除标点符号\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 分词\n",
    "    words = word_tokenize(text)\n",
    "    # 移除停用词和单字词\n",
    "    words = [word for word in words if word not in stopwords.words('english') and len(word) > 1]\n",
    "    # 词形还原\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # 重组为处理后的文本\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Load data\n",
    "file_path = '/Users/xiao1/Desktop/LDMar10.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Preprocess text data\n",
    "df['processed_text'] = df[\"Sentences with 'Key Words'\"].apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['processed_text'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text'].values)\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['Level'])\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32, input_length=100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {evaluation[0]}')\n",
    "print(f'Test Accuracy: {evaluation[1]}')\n",
    "\n",
    "# Save model, tokenizer, and label encoder\n",
    "model.save('/Users/xiao1/Desktop/mymodel.h5')\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('/Users/xiao1/Desktop/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "joblib.dump(label_encoder, '/Users/xiao1/Desktop/label_encoder.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可用版本 但不匹配bert纬度\n",
    "维度不匹配：BERT模型生成的特征向量维度是768维，而您的RNN模型期望的输入是一个长度为100的序列。它们的维度不匹配。\n",
    "数据类型不匹配：BERT模型输出的是密集的实数向量，而RNN模型被训练为接受整数索引序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 2.0038 - accuracy: 0.1087 - val_loss: 1.9287 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.7424 - accuracy: 0.5761 - val_loss: 1.8049 - val_accuracy: 0.3750\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5876 - accuracy: 0.7609 - val_loss: 1.8649 - val_accuracy: 0.1250\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5082 - accuracy: 0.7065 - val_loss: 1.3576 - val_accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2686 - accuracy: 0.7826 - val_loss: 1.4355 - val_accuracy: 0.7917\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1552 - accuracy: 0.9239 - val_loss: 1.2732 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1097 - accuracy: 0.8043 - val_loss: 1.4076 - val_accuracy: 0.7917\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1002 - accuracy: 0.9457 - val_loss: 1.4625 - val_accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0439 - accuracy: 0.9565 - val_loss: 1.8573 - val_accuracy: 0.1667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2451 - accuracy: 0.9130 - val_loss: 1.2738 - val_accuracy: 0.7917\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4944 - accuracy: 0.6552\n",
      "Test Loss: 1.4944121837615967\n",
      "Test Accuracy: 0.6551724076271057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/xiao1/Desktop/label_encoder.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "import joblib\n",
    "\n",
    "# Load data\n",
    "file_path = '/Users/xiao1/Desktop/LDMar10.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Data preprocessing\n",
    "df[\"Sentences with 'Key Words'\"] = df[\"Sentences with 'Key Words'\"].fillna(\"\").astype(str)\n",
    "texts = df[\"Sentences with 'Key Words'\"].tolist()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['Level'])\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32, input_length=100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(len(set(encoded_labels)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {evaluation[0]}')\n",
    "print(f'Test Accuracy: {evaluation[1]}')\n",
    "\n",
    "# Save model, tokenizer, and label encoder\n",
    "model.save('/Users/xiao1/Desktop/model.h5')  # 更改为可写的路径\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('/Users/xiao1/Desktop/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "joblib.dump(label_encoder, '/Users/xiao1/Desktop/label_encoder.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/xiao1/Desktop/LDMar10.xlsx'\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "df = pd.read_excel(file_path)\n",
    "# Clean and prepare the data\n",
    "df[\"Sentences with 'Key Words'\"] = df[\"Sentences with 'Key Words'\"].fillna(\"\").astype(str)\n",
    "texts = df[\"Sentences with 'Key Words'\"].tolist()\n",
    "\n",
    "# The rest of your code here...\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['Level'])\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RNN model with output shape according to the number of unique labels\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32, input_length=100))\n",
    "model.add(SimpleRNN(32))  # You might use LSTM or GRU for better performance\n",
    "model.add(Dense(len(set(encoded_labels)), activation='softmax'))  # Multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {evaluation[0]}')\n",
    "print(f'Test Accuracy: {evaluation[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 错误版第二版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/xiao1/Desktop/LDMar10.xlsx'\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 加载数据\n",
    "file_path = '/Users/xiao1/Desktop/LDMar10.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 确保所有文本数据都是字符串类型，用空字符串填充缺失值\n",
    "df[\"Sentences with 'Key Words'\"] = df[\"Sentences with 'Key Words'\"].fillna(\"\").astype(str)\n",
    "\n",
    "# 初始化Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 使用LabelEncoder对标签进行编码\n",
    "label_encoder = LabelEncoder()\n",
    "df['Encoded Labels'] = label_encoder.fit_transform(df['Level'])\n",
    "\n",
    "# 文本和标签\n",
    "texts = df[\"Sentences with 'Key Words'\"].values\n",
    "labels = df['Encoded Labels'].values\n",
    "\n",
    "# 将标签转换为one-hot编码\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# 分割数据集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.1, random_state=2021)\n",
    "\n",
    "# 使用BERT的Tokenizer处理文本数据\n",
    "max_length = 64\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "# 初始化BERT模型\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# 设置优化器、损失函数和评价指标\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
